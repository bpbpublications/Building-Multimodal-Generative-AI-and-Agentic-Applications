{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Human in Loop"
      ],
      "metadata": {
        "id": "d7fhEpQALjP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Okl0SwjLIaT"
      },
      "outputs": [],
      "source": [
        "# main.py\n",
        "from orchestrator.langgraph_flow import run_rag_workflow\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Human-in-the-Loop RAG System Ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        query = input(\"\\nUser: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        answer = run_rag_workflow(query)\n",
        "        print(\"\\nAssistant:\", answer)\n",
        "\n",
        "# config.py\n",
        "MODEL_NAME = \"mistral\"\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
        "VECTOR_DB_PATH = \"db\"\n",
        "SOURCE_DOC = \"data/source_docs/ai_education_article.pdf\"\n",
        "\n",
        "# requirements.txt\n",
        "langchain\n",
        "langchain-community\n",
        "chromadb\n",
        "ollama\n",
        "reportlab\n",
        "rank_bm25\n",
        "langgraph\n",
        "\n",
        "# llm/generate.py\n",
        "from langchain_community.llms import Ollama\n",
        "from config import MODEL_NAME\n",
        "\n",
        "def get_llm():\n",
        "    return Ollama(model=MODEL_NAME, temperature=0.2)\n",
        "\n",
        "# llm/react_prompt.py\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "react_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are an intelligent assistant using the ReAct (Reasoning + Acting) technique.\n",
        "Break down the user query into reasoning steps and retrieve relevant information accordingly.\n",
        "\n",
        "Question: {question}\n",
        "Relevant Context:\n",
        "{context}\n",
        "\n",
        "First, list your reasoning steps clearly.\n",
        "Then, provide a final answer based on those steps and the retrieved context.\n",
        "\n",
        "Reasoning Steps:\n",
        "1.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# embeddings/embedder.py\n",
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from config import EMBEDDING_MODEL\n",
        "\n",
        "def get_embedding_model():\n",
        "    return OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
        "\n",
        "# vectorstore/db_handler.py\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from embeddings.embedder import get_embedding_model\n",
        "from config import VECTOR_DB_PATH\n",
        "\n",
        "def get_vectorstore(documents):\n",
        "    embedding_model = get_embedding_model()\n",
        "    return Chroma.from_documents(documents, embedding=embedding_model, persist_directory=VECTOR_DB_PATH)\n",
        "\n",
        "# vectorstore/metadata_schema.py\n",
        "def add_metadata_to_chunks(chunks, source_name):\n",
        "    for chunk in chunks:\n",
        "        if not chunk.metadata:\n",
        "            chunk.metadata = {}\n",
        "        chunk.metadata[\"source\"] = source_name\n",
        "    return chunks\n",
        "\n",
        "# retriever/hybrid_search.py\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "def get_hybrid_retriever(chunks, vectorstore):\n",
        "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "    bm25_retriever.k = 4\n",
        "    vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "    return EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "# parser/pdf_parser.py\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from config import SOURCE_DOC\n",
        "from vectorstore.metadata_schema import add_metadata_to_chunks\n",
        "import os\n",
        "\n",
        "def load_and_chunk_pdf():\n",
        "    loader = PyPDFLoader(SOURCE_DOC)\n",
        "    documents = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "    return add_metadata_to_chunks(chunks, os.path.basename(SOURCE_DOC))\n",
        "\n",
        "# memory/conversation_buffer.py\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# grader/doc_relevance.py\n",
        "def is_relevant(doc, question):\n",
        "    return question.lower() in doc.page_content.lower()\n",
        "\n",
        "# grader/hallucination.py\n",
        "def is_grounded(answer, documents):\n",
        "    return any(doc.page_content.lower() in answer.lower() for doc in documents)\n",
        "\n",
        "# grader/human_feedback.py\n",
        "def human_approval_required():\n",
        "    return input(\"\\nApprove the answer? (yes/no): \").strip().lower() != \"yes\"\n",
        "\n",
        "# orchestrator/langgraph_flow.py\n",
        "from parser.pdf_parser import load_and_chunk_pdf\n",
        "from vectorstore.db_handler import get_vectorstore\n",
        "from retriever.hybrid_search import get_hybrid_retriever\n",
        "from llm.generate import get_llm\n",
        "from llm.react_prompt import react_prompt\n",
        "from memory.conversation_buffer import memory\n",
        "from utils.cite_sources import format_sources\n",
        "from grader.human_feedback import human_approval_required\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "def run_rag_workflow(question):\n",
        "    chunks = load_and_chunk_pdf()\n",
        "    vectorstore = get_vectorstore(chunks)\n",
        "    retriever = get_hybrid_retriever(chunks, vectorstore)\n",
        "    llm = get_llm()\n",
        "    rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "        combine_docs_chain_kwargs={\"prompt\": react_prompt},\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    retries = 3\n",
        "    for attempt in range(retries):\n",
        "        result = rag_chain.invoke({\"question\": question})\n",
        "        sources = format_sources(result.get(\"source_documents\", []))\n",
        "\n",
        "        print(\"\\nSources:\")\n",
        "        for src in sources:\n",
        "            print(\"-\", src)\n",
        "\n",
        "        print(\"\\nGenerated Answer:\")\n",
        "        print(result[\"answer\"])\n",
        "\n",
        "        if not human_approval_required():\n",
        "            return result[\"answer\"]\n",
        "\n",
        "        print(\"\\nRetrying with same question...\")\n",
        "\n",
        "    return \"Answer rejected after multiple attempts.\"\n",
        "\n",
        "# utils/cite_sources.py\n",
        "def format_sources(source_documents):\n",
        "    return [doc.metadata.get(\"source\", \"[unknown]\") for doc in source_documents]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 2: Multi agent Human in loop"
      ],
      "metadata": {
        "id": "TL863nOfMrQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from orchestrator.langgraph_flow import graph\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Multi-Agent Human-in-the-Loop RAG System Ready. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        query = input(\"\\nUser: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        result = graph.invoke({\"question\": query})\n",
        "        print(\"\\nAssistant:\", result.get(\"answer\", \"[No answer generated]\"))\n",
        "\n",
        "# config.py\n",
        "MODEL_NAME = \"mistral\"\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
        "VECTOR_DB_PATH = \"db\"\n",
        "SOURCE_DOC = \"data/source_docs/ai_education_article.pdf\"\n",
        "\n",
        "# requirements.txt\n",
        "langchain\n",
        "langchain-community\n",
        "chromadb\n",
        "ollama\n",
        "reportlab\n",
        "rank_bm25\n",
        "langgraph\n",
        "\n",
        "# llm/generate.py\n",
        "from langchain_community.llms import Ollama\n",
        "from config import MODEL_NAME\n",
        "\n",
        "def get_llm():\n",
        "    return Ollama(model=MODEL_NAME, temperature=0.2)\n",
        "\n",
        "# llm/react_prompt.py\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "react_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are an intelligent assistant using the ReAct (Reasoning + Acting) technique.\n",
        "Break down the user query into reasoning steps and retrieve relevant information accordingly.\n",
        "\n",
        "Question: {question}\n",
        "Relevant Context:\n",
        "{context}\n",
        "\n",
        "First, list your reasoning steps clearly.\n",
        "Then, provide a final answer based on those steps and the retrieved context.\n",
        "\n",
        "Reasoning Steps:\n",
        "1.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# embeddings/embedder.py\n",
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from config import EMBEDDING_MODEL\n",
        "\n",
        "def get_embedding_model():\n",
        "    return OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
        "\n",
        "# vectorstore/db_handler.py\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from embeddings.embedder import get_embedding_model\n",
        "from config import VECTOR_DB_PATH\n",
        "\n",
        "def get_vectorstore(documents):\n",
        "    embedding_model = get_embedding_model()\n",
        "    return Chroma.from_documents(documents, embedding=embedding_model, persist_directory=VECTOR_DB_PATH)\n",
        "\n",
        "# vectorstore/metadata_schema.py\n",
        "def add_metadata_to_chunks(chunks, source_name):\n",
        "    for chunk in chunks:\n",
        "        if not chunk.metadata:\n",
        "            chunk.metadata = {}\n",
        "        chunk.metadata[\"source\"] = source_name\n",
        "    return chunks\n",
        "\n",
        "# retriever/hybrid_search.py\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "def get_hybrid_retriever(chunks, vectorstore):\n",
        "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
        "    bm25_retriever.k = 4\n",
        "    vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "    return EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "# parser/pdf_parser.py\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from config import SOURCE_DOC\n",
        "from vectorstore.metadata_schema import add_metadata_to_chunks\n",
        "import os\n",
        "\n",
        "def load_and_chunk_pdf():\n",
        "    loader = PyPDFLoader(SOURCE_DOC)\n",
        "    documents = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "    return add_metadata_to_chunks(chunks, os.path.basename(SOURCE_DOC))\n",
        "\n",
        "# memory/conversation_buffer.py\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# grader/doc_relevance.py\n",
        "def is_relevant(doc, question):\n",
        "    return question.lower() in doc.page_content.lower()\n",
        "\n",
        "# grader/hallucination.py\n",
        "def is_grounded(answer, documents):\n",
        "    return any(doc.page_content.lower() in answer.lower() for doc in documents)\n",
        "\n",
        "# grader/human_feedback.py\n",
        "def human_approval_required():\n",
        "    return input(\"\n",
        "Approve the answer? (yes/no): \").strip().lower() != \"yes\"\n",
        "\n",
        "# utils/cite_sources.py\n",
        "def format_sources(source_documents):\n",
        "    return [doc.metadata.get(\"source\", \"[unknown]\") for doc in source_documents]\n",
        "\n",
        "# orchestrator/langgraph_flow.py\n",
        "from parser.pdf_parser import load_and_chunk_pdf\n",
        "from vectorstore.db_handler import get_vectorstore\n",
        "from retriever.hybrid_search import get_hybrid_retriever\n",
        "from llm.generate import get_llm\n",
        "from llm.react_prompt import react_prompt\n",
        "from memory.conversation_buffer import memory\n",
        "from utils.cite_sources import format_sources\n",
        "from grader.human_feedback import human_approval_required\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# Setup shared components\n",
        "chunks = load_and_chunk_pdf()\n",
        "vectorstore = get_vectorstore(chunks)\n",
        "retriever = get_hybrid_retriever(chunks, vectorstore)\n",
        "llm = get_llm()\n",
        "\n",
        "rag_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    combine_docs_chain_kwargs={\"prompt\": react_prompt},\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# Define LangGraph nodes as agents\n",
        "def retrieval_agent(state):\n",
        "    print(\"\\n[Retrieval Agent Invoked]\")\n",
        "    return {\"documents\": retriever.get_relevant_documents(state[\"question\"])}\n",
        "\n",
        "def generation_agent(state):\n",
        "    print(\"\\n[Generation Agent Invoked]\")\n",
        "    result = rag_chain.invoke({\"question\": state[\"question\"]})\n",
        "    sources = format_sources(result.get(\"source_documents\", []))\n",
        "    print(\"\\nSources:\")\n",
        "    for src in sources:\n",
        "        print(\"-\", src)\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(result[\"answer\"])\n",
        "    return {\"answer\": result[\"answer\"], \"source_documents\": result.get(\"source_documents\", [])}\n",
        "\n",
        "def human_feedback_agent(state):\n",
        "    print(\"\\n[Human Validator Agent Invoked]\")\n",
        "    approved = not human_approval_required()\n",
        "    return {\"approved\": approved}\n",
        "\n",
        "# Define state graph\n",
        "class GraphState(dict):\n",
        "    question: str\n",
        "    documents: list\n",
        "    answer: str\n",
        "    approved: bool\n",
        "    source_documents: list\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"retrieve\", retrieval_agent)\n",
        "workflow.add_node(\"generate\", generation_agent)\n",
        "workflow.add_node(\"validate\", human_feedback_agent)\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.add_edge(\"generate\", \"validate\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"validate\",\n",
        "    lambda state: \"end\" if state.get(\"approved\") else \"generate\",\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"generate\": \"generate\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph = workflow.compile()\n"
      ],
      "metadata": {
        "id": "IhTpVk36Mtd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}